{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 5**\n",
        "\n",
        "**This exercise aims to provide a comprehensive learning experience in text analysis and machine learning techniques, focusing on both text classification and clustering tasks.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks***.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ],
      "metadata": {
        "id": "TU-pLW33lpcS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## **Question 1 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text classification** as well as the performance evaluation. In addition, you are requried to conduct **10 fold cross validation** (https://scikit-learn.org/stable/modules/cross_validation.html) in the training.\n",
        "\n",
        "\n",
        "\n",
        "The dataset can be download from canvas. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algorithms:**\n",
        "\n",
        "*   MultinominalNB\n",
        "*   SVM\n",
        "*   KNN\n",
        "*   Decision tree\n",
        "*   Random Forest\n",
        "*   XGBoost\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "**Evaluation measurement:**\n",
        "\n",
        "\n",
        "*   Accuracy\n",
        "*   Recall\n",
        "*   Precison\n",
        "*   F-1 score\n"
      ],
      "metadata": {
        "id": "loi8Sh7UE6ha"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ed33be2-df3c-441e-a4f1-a2ba41a8dc1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> x\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> u\n",
            "\n",
            "Nothing to update.\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "#Importing the required libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "with open(\"stsa-train.txt\") as txtf:\n",
        "    mylist = [line.rstrip('\\n') for line in txtf]\n",
        "\n",
        "labels = []\n",
        "text = []\n",
        "\n",
        "for i, line in enumerate(mylist):\n",
        "    label = mylist[i][0]\n",
        "    tex = mylist[i][1:]\n",
        "    labels.append(label)\n",
        "    text.append(tex)\n",
        "\n",
        "train_dataset = pd.DataFrame(list(zip(labels, text)),columns =['Reviews', 'Text'])\n",
        "train_dataset.head()\n",
        "\n",
        "import nltk\n",
        "nltk.download()\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess(sentence):\n",
        "    sentence=str(sentence)\n",
        "    sentence = sentence.lower()\n",
        "    sentence=sentence.replace('{html}',\"\")\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, '', sentence)\n",
        "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
        "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(rem_num)\n",
        "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
        "    stem_words=[stemmer.stem(w) for w in filtered_words]\n",
        "    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "\n",
        "train_dataset['cleanText']=train_dataset['Text'].map(lambda s:preprocess(s))\n",
        "train_dataset.head()\n",
        "\n",
        "with open(\"stsa-test.txt\") as txtf:\n",
        "    mylist_test_data = [line.rstrip('\\n') for line in txtf]\n",
        "\n",
        "labels_test = []\n",
        "text_test = []\n",
        "\n",
        "for i, line in enumerate(mylist_test_data):\n",
        "    label_test = mylist_test_data[i][0]\n",
        "    tex_test = mylist_test_data[i][1:]\n",
        "    labels_test.append(label_test)\n",
        "    text_test.append(tex_test)\n",
        "\n",
        "test_dataset = pd.DataFrame(list(zip(labels_test, text_test)),columns =['Reviews', 'Text'])\n",
        "test_dataset.head()\n",
        "\n",
        "test_dataset['cleanText']=test_dataset['Text'].map(lambda s:preprocess(s))\n",
        "test_dataset.head()\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(lowercase = False, analyzer='word')\n",
        "tfIDF_train = tfidf_vectorizer.fit_transform(train_dataset[\"cleanText\"]).toarray()\n",
        "tfIDF_test = tfidf_vectorizer.transform(test_dataset[\"cleanText\"]).toarray()\n",
        "\n",
        "x_test = tfIDF_test\n",
        "y_test = test_dataset[\"Reviews\"]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(tfIDF_train,train_dataset[\"Reviews\"],test_size = 0.2, random_state = 85)\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_model = nb_classifier.fit(x_train, y_train)\n",
        "predictions_validation_set = nb_classifier.predict(x_valid)\n",
        "\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "print (\"Accuracy of Multinominal  Naive Bayes model  : \", round(accuracy_score(y_valid, predictions_validation_set)*100),\"%\")\n",
        "print (\"Percision of Multinominal Naive Bayes model  : \", round(precision_score(y_valid, predictions_validation_set, pos_label='0')*100),\"%\")\n",
        "print (\"Recall of Multinominal Naive Bayes model  : \", round(recall_score(y_valid, predictions_validation_set, pos_label='0')*100),\"%\")\n",
        "print (\"F1 Score of Multinominal Naive Bayes model  : \", round(f1_score(y_valid, predictions_validation_set, pos_label='0')*100),\"%\")\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "classification_Report_naive_bayes = classification_report(y_valid, predictions_validation_set)\n",
        "print(\"Classification Report: \", \"\\n\", \"\\n\",classification_Report_naive_bayes)\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "naive_accuracies_validation = cross_val_score(estimator = nb_classifier, X = x_train, y = y_train, cv = 10)\n",
        "\n",
        "print(f\"Naive Bayes Model  10-fold cross validation score on training set is :  {round(naive_accuracies_validation.mean()*100)}%\")\n",
        "\n",
        "predictions_test_set = nb_classifier.predict(x_test)\n",
        "print (\"Accuracy of the Naive Bayes model on test set is : \", round(accuracy_score(y_test, predictions_test_set)*100),\"%\")\n",
        "print (\"Percision of the Naive Bayes model on validation set is : \", round(precision_score(y_test, predictions_test_set, pos_label='0')*100),\"%\")\n",
        "print (\"Recall of the Naive Bayes model on validation set is : \", round(recall_score(y_test, predictions_test_set, pos_label='0')*100),\"%\")\n",
        "print (\"F1 Score of the Naive Bayes model on validation set is : \", round(f1_score(y_test, predictions_test_set, pos_label='0')*100),\"%\")\n",
        "\n",
        "classification_Report_naive_bayes_Test_data = classification_report(y_test, predictions_test_set)\n",
        "print(\"Classification Report: \", \"\\n\", \"\\n\",classification_Report_naive_bayes_Test_data)\n",
        "\n",
        "naive_accuracies_test = cross_val_score(estimator = nb_classifier, X = x_test, y = y_test, cv = 10)\n",
        "\n",
        "print(f\"Naive Bayes Model 10-fold cross validation score on testing set is :  {round(naive_accuracies_test.mean()*100)}%\")\n",
        "\n",
        "from sklearn import svm\n",
        "classifier_svm = svm.SVC()\n",
        "model_svm = classifier_svm.fit(x_train, y_train)\n",
        "svm_predictions_validation_set = classifier_svm.predict(x_valid)\n",
        "\n",
        "print (\"Accuracy of the SVM model on validation set is : \", round(accuracy_score(y_valid, svm_predictions_validation_set)*100),\"%\")\n",
        "print (\"Percision of the SVM model on validation set is : \", round(precision_score(y_valid, svm_predictions_validation_set, pos_label='0')*100),\"%\")\n",
        "print (\"Recall of the SVM model on validation set is : \", round(recall_score(y_valid, svm_predictions_validation_set, pos_label='0')*100),\"%\")\n",
        "print (\"F1 Score of the SVM model on validation set is : \", round(f1_score(y_valid, svm_predictions_validation_set, pos_label='0')*100),\"%\")\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "svm_validation_Classification_report = classification_report(y_valid, svm_predictions_validation_set)\n",
        "print(\"Classification Report: \", \"\\n\", \"\\n\",svm_validation_Classification_report)\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "svm_accuracies_validation = cross_val_score(estimator = classifier_svm, X = x_train, y = y_train, cv = 10)\n",
        "\n",
        "print(f\"SVM Model  10-fold cross validation score on training set is :  {round(svm_accuracies_validation.mean()*100)}%\")\n",
        "\n",
        "svm_predictions_test_set = classifier_svm.predict(x_test)\n",
        "print (\"Accuracy of the SVM model on test set is : \", round(accuracy_score(y_test, svm_predictions_test_set)*100),\"%\")\n",
        "print (\"Percision of the SVM model on validation set is : \", round(precision_score(y_test, svm_predictions_test_set, pos_label='0')*100),\"%\")\n",
        "print (\"Recall of the SVM model on validation set is : \", round(recall_score(y_test, svm_predictions_test_set, pos_label='0')*100),\"%\")\n",
        "print (\"F1 Score of the SVM model on validation set is : \", round(f1_score(y_test, svm_predictions_test_set, pos_label='0')*100),\"%\")\n",
        "\n",
        "svm_predictions_test_set = classifier_svm.predict(x_test)\n",
        "print (\"Accuracy of the SVM model on test set is : \", round(accuracy_score(y_test, svm_predictions_test_set)*100),\"%\")\n",
        "print (\"Percision of the SVM model on validation set is : \", round(precision_score(y_test, svm_predictions_test_set, pos_label='0')*100),\"%\")\n",
        "print (\"Recall of the SVM model on validation set is : \", round(recall_score(y_test, svm_predictions_test_set, pos_label='0')*100),\"%\")\n",
        "print (\"F1 Score of the SVM model on validation set is : \", round(f1_score(y_test, svm_predictions_test_set, pos_label='0')*100),\"%\")\n",
        "\n",
        "svm_test_validation_Classification_report = classification_report(y_test, svm_predictions_test_set)\n",
        "print(\"Classification Report: \", \"\\n\", \"\\n\",svm_test_validation_Classification_report)\n",
        "\n",
        "svm_accuracies_test = cross_val_score(estimator = classifier_svm, X = x_test, y = y_test, cv = 10)\n",
        "\n",
        "print(f\"SVM Model 10-fold cross validation score on testing set is :  {round(svm_accuracies_test.mean()*100)}%\")\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "classifier_knn = KNeighborsClassifier(n_neighbors = 15)\n",
        "model_knn = classifier_knn.fit(x_train, y_train)\n",
        "knn_predictions_validation_set = classifier_knn.predict(x_valid)\n",
        "\n",
        "print (\"Accuracy of the KNN model on validation set is : \", round(accuracy_score(y_valid, knn_predictions_validation_set)*100),\"%\")\n",
        "print (\"Percision of the KNN model on validation set is : \", round(precision_score(y_valid, knn_predictions_validation_set, pos_label='0')*100),\"%\")\n",
        "print (\"Recall of the KNN model on validation set is : \", round(recall_score(y_valid, knn_predictions_validation_set, pos_label='0')*100),\"%\")\n",
        "print (\"F1 Score of the KNN model on validation set is : \", round(f1_score(y_valid, knn_predictions_validation_set, pos_label='0')*100),\"%\")\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "knn_validation_Classification_report = classification_report(y_valid, knn_predictions_validation_set)\n",
        "print(\"Classification Report: \", \"\\n\", \"\\n\",knn_validation_Classification_report)\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "knn_accuracies_validation = cross_val_score(estimator = classifier_knn, X = x_train, y = y_train, cv = 10)\n",
        "\n",
        "print(f\"KNN Model  10-fold cross validation score on training set is :  {round(knn_accuracies_validation.mean()*100)}%\")\n",
        "\n",
        "knn_predictions_test_set = classifier_knn.predict(x_test)\n",
        "print (\"Accuracy of the KNN model on test set is : \", round(accuracy_score(y_test, knn_predictions_test_set)*100),\"%\")\n",
        "print (\"Percision of the KNN model on validation set is : \", round(precision_score(y_test, knn_predictions_test_set, pos_label='0')*100),\"%\")\n",
        "print (\"Recall of the KNN model on validation set is : \", round(recall_score(y_test, knn_predictions_test_set, pos_label='0')*100),\"%\")\n",
        "print (\"F1 Score of the KNN model on validation set is : \", round(f1_score(y_test, knn_predictions_test_set, pos_label='0')*100),\"%\")\n",
        "\n",
        "knn_test_validation_Classification_report = classification_report(y_test, knn_predictions_test_set)\n",
        "print(\"Classification Report: \", \"\\n\", \"\\n\",knn_test_validation_Classification_report)\n",
        "\n",
        "knn_accuracies_test = cross_val_score(estimator = classifier_knn, X = x_test, y = y_test, cv = 10)\n",
        "\n",
        "print(f\"KNN Model 10-fold cross validation score on testing set is :  {round(knn_accuracies_test.mean()*100)}%\")\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "classifier_dt = DecisionTreeClassifier()\n",
        "model_dt = classifier_dt.fit(x_train, y_train)\n",
        "dt_predictions_validation_set = classifier_dt.predict(x_valid)\n",
        "\n",
        "print (\"Accuracy of the Decison Tree Classifier model on validation set is : \", round(accuracy_score(y_valid, dt_predictions_validation_set)*100),\"%\")\n",
        "print (\"Percision of the Decison Tree Classifier model on validation set is : \", round(precision_score(y_valid, dt_predictions_validation_set, pos_label='0')*100),\"%\")\n",
        "print (\"Recall of the Decison Tree Classifier model on validation set is : \", round(recall_score(y_valid, dt_predictions_validation_set, pos_label='0')*100),\"%\")\n",
        "print (\"F1 Score of the Decison Tree Classifier model on validation set is : \", round(f1_score(y_valid, dt_predictions_validation_set, pos_label='0')*100),\"%\")\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "dt_validation_Classification_report = classification_report(y_valid, dt_predictions_validation_set)\n",
        "print(\"Classification Report: \", \"\\n\", \"\\n\",dt_validation_Classification_report)\n",
        "\n",
        "dt_predictions_test_set = classifier_dt.predict(x_test)\n",
        "print (\"Accuracy of the Decison Tree Classifier model on test set is : \", round(accuracy_score(y_test, dt_predictions_test_set)*100),\"%\")\n",
        "print (\"Percision of the Decison Tree Classifier model on validation set is : \", round(precision_score(y_test, dt_predictions_test_set, pos_label='0')*100),\"%\")\n",
        "print (\"Recall of the Decison Tree Classifier model on validation set is : \", round(recall_score(y_test, dt_predictions_test_set, pos_label='0')*100),\"%\")\n",
        "print (\"F1 Score of the Decison Tree Classifier model on validation set is : \", round(f1_score(y_test, dt_predictions_test_set, pos_label='0')*100),\"%\")\n",
        "\n",
        "dt_validation_Classification_report_test = classification_report(y_test, dt_predictions_test_set)\n",
        "print(\"Classification Report: \", \"\\n\", \"\\n\",dt_validation_Classification_report_test)\n",
        "\n",
        "dt_accuracies_test = cross_val_score(estimator = classifier_dt, X = x_test, y = y_test, cv = 10)\n",
        "\n",
        "print(f\"Decison Tree Classifier Model 10-fold cross validation score on testing set is :  {round(dt_accuracies_test.mean()*100)}%\")\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "dt_accuracies_validation = cross_val_score(estimator = classifier_dt, X = x_train, y = y_train, cv = 10)\n",
        "\n",
        "print(f\"Decison Tree Classifier Model  10-fold cross validation score on training set is :  {round(dt_accuracies_validation.mean()*100)}%\")\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "classifier_rf = RandomForestClassifier()\n",
        "model_rf = classifier_rf.fit(x_train, y_train)\n",
        "rf_predictions_validation_set = classifier_rf.predict(x_valid)\n",
        "\n",
        "print (\"Accuracy of the Random Forest Classifier model on validation set is : \", round(accuracy_score(y_valid, rf_predictions_validation_set)*100),\"%\")\n",
        "print (\"Percision of the Random Forest Classifier model on validation set is : \", round(precision_score(y_valid, rf_predictions_validation_set, pos_label='0')*100),\"%\")\n",
        "print (\"Recall of the Random Forest Classifier model on validation set is : \", round(recall_score(y_valid, rf_predictions_validation_set, pos_label='0')*100),\"%\")\n",
        "print (\"F1 Score of the Random Forest Classifier model on validation set is : \", round(f1_score(y_valid, rf_predictions_validation_set, pos_label='0')*100),\"%\")\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "rf_validation_Classification_report = classification_report(y_valid, rf_predictions_validation_set)\n",
        "print(\"Classification Report: \", \"\\n\", \"\\n\",rf_validation_Classification_report)\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "rf_accuracies_validation = cross_val_score(estimator = classifier_rf, X = x_train, y = y_train, cv = 10)\n",
        "\n",
        "print(f\"Decison Random Forest Model  10-fold cross validation score on training set is :  {round(rf_accuracies_validation.mean()*100)}%\")\n",
        "\n",
        "rf_predictions_test_set = classifier_rf.predict(x_test)\n",
        "print (\"Accuracy of the Random Forest Classifier model on test set is : \", round(accuracy_score(y_test, rf_predictions_test_set)*100),\"%\")\n",
        "print (\"Percision of the Random Forest Classifier model on validation set is : \", round(precision_score(y_test, rf_predictions_test_set, pos_label='0')*100),\"%\")\n",
        "print (\"Recall of the Random Forest Classifier model on validation set is : \", round(recall_score(y_test, rf_predictions_test_set, pos_label='0')*100),\"%\")\n",
        "print (\"F1 Score of the Random Forest Classifier model on validation set is : \", round(f1_score(y_test, rf_predictions_test_set, pos_label='0')*100),\"%\")\n",
        "\n",
        "rf_validation_Classification_report_test = classification_report(y_test, rf_predictions_test_set)\n",
        "print(\"Classification Report: \", \"\\n\", \"\\n\",rf_validation_Classification_report_test)\n",
        "\n",
        "rf_accuracies_test = cross_val_score(estimator = classifier_rf, X = x_test, y = y_test, cv = 10)\n",
        "\n",
        "print(f\"Random Forest Classifier Model 10-fold cross validation score on testing set is :  {round(rf_accuracies_test.mean()*100)}%\")\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "rf_accuracies_validation = cross_val_score(estimator = classifier_rf, X = x_train, y = y_train, cv = 10)\n",
        "\n",
        "print(f\"Decison Random Forest Model  10-fold cross validation score on training set is :  {round(rf_accuracies_validation.mean()*100)}%\")\n",
        "\n",
        "rf_predictions_test_set = classifier_rf.predict(x_test)\n",
        "print (\"Accuracy of the Random Forest Classifier model on test set is : \", round(accuracy_score(y_test, rf_predictions_test_set)*100),\"%\")\n",
        "print (\"Percision of the Random Forest Classifier model on validation set is : \", round(precision_score(y_test, rf_predictions_test_set, pos_label='0')*100),\"%\")\n",
        "print (\"Recall of the Random Forest Classifier model on validation set is : \", round(recall_score(y_test, rf_predictions_test_set, pos_label='0')*100),\"%\")\n",
        "print (\"F1 Score of the Random Forest Classifier model on validation set is : \", round(f1_score(y_test, rf_predictions_test_set, pos_label='0')*100),\"%\")\n",
        "\n",
        "cr_rf_test = classification_report(y_test, rf_predictions_test_set)\n",
        "print(\"Classification Report: \", \"\\n\", \"\\n\",cr_rf_test)\n",
        "\n",
        "rf_accuracies_test = cross_val_score(estimator = classifier_rf, X = x_test, y = y_test, cv = 10)\n",
        "\n",
        "print(f\"Random Forest Classifier Model 10-fold cross validation score on testing set is :  {round(rf_accuracies_test.mean()*100)}%\")\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "classifier_xgb = XGBClassifier()\n",
        "model_xgb = classifier_xgb.fit(x_train, y_train)\n",
        "xgb_predictions_validation_set = classifier_xgb.predict(x_valid)\n",
        "\n",
        "print (\"Accuracy of the XGBoost Classifier model on validation set is : \", round(accuracy_score(y_valid, xgb_predictions_validation_set)*100),\"%\")\n",
        "print (\"Percision of the XGBoost Classifier model on validation set is : \", round(precision_score(y_valid, xgb_predictions_validation_set, pos_label='0')*100),\"%\")\n",
        "print (\"Recall of the XGBoost Classifier model on validation set is : \", round(recall_score(y_valid, xgb_predictions_validation_set, pos_label='0')*100),\"%\")\n",
        "print (\"F1 Score of the XGBoost Classifier model on validation set is : \", round(f1_score(y_valid, xgb_predictions_validation_set, pos_label='0')*100),\"%\")\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "cr_xgb_validation = classification_report(y_valid, xgb_predictions_validation_set)\n",
        "print(\"Classification Report: \", \"\\n\", \"\\n\",cr_xgb_validation)\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "xgb_accuracies_validation = cross_val_score(estimator = classifier_xgb, X = x_train, y = y_train, cv = 10)\n",
        "\n",
        "print(f\"XGBoost Model  10-fold cross validation score on training set is :  {round(xgb_accuracies_validation.mean()*100)}%\")\n",
        "\n",
        "\n",
        "xgb_predictions_test_set = classifier_xgb.predict(x_test)\n",
        "print (\"Accuracy of the XGBoost Classifier model on test set is : \", round(accuracy_score(y_test, xgb_predictions_test_set)*100),\"%\")\n",
        "print (\"Percision of the XGBoost Classifier model on validation set is : \", round(precision_score(y_test, xgb_predictions_test_set, pos_label='0')*100),\"%\")\n",
        "print (\"Recall of the XGBoost Classifier model on validation set is : \", round(recall_score(y_test, xgb_predictions_test_set, pos_label='0')*100),\"%\")\n",
        "print (\"F1 Score of the XGBoost Classifier model on validation set is : \", round(f1_score(y_test, xgb_predictions_test_set, pos_label='0')*100),\"%\")\n",
        "\n",
        "cr_xgb_test = classification_report(y_test, xgb_predictions_test_set)\n",
        "print(\"Classification Report: \", \"\\n\", \"\\n\",cr_xgb_test)\n",
        "\n",
        "xgb_accuracies_test = cross_val_score(estimator = classifier_xgb, X = x_test, y = y_test, cv = 10)\n",
        "\n",
        "print(f\"XGBoost Classifier Model 10-fold cross validation score on testing set is :  {round(xgb_accuracies_test.mean()*100)}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## **Question 2 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text clustering**.\n",
        "\n",
        "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
        "(You can also use different text data which you want)\n",
        "\n",
        "**Apply the listed clustering methods to the dataset:**\n",
        "*   K-means\n",
        "*   DBSCAN\n",
        "*   Hierarchical clustering\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "You can refer to of the codes from  the follwing link below.\n",
        "https://www.kaggle.com/karthik3890/text-clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "c3ab35eb-459e-4b67-d132-95eaa6fd854e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-5bcd2cbc35a4>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Amazon_Unlocked_Mobile.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Reviews'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Reviews'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, arg, na_action)\u001b[0m\n\u001b[1;32m   4395\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4396\u001b[0m         \"\"\"\n\u001b[0;32m-> 4397\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4398\u001b[0m         return self._constructor(new_values, index=self.index, copy=False).__finalize__(\n\u001b[1;32m   4399\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"map\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0;31m# mapper is a function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-5bcd2cbc35a4>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Amazon_Unlocked_Mobile.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Reviews'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Reviews'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-19be7b32af20>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrem_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mfiltered_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mstem_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiltered_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mlemma_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstem_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-19be7b32af20>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrem_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mfiltered_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mstem_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiltered_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mlemma_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstem_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36mstem\u001b[0;34m(self, word, to_lowercase)\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step1a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step1b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step1c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36m_step1c\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_contains_vowel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         return self._apply_rule_list(\n\u001b[0m\u001b[1;32m    423\u001b[0m             \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             [\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36m_apply_rule_list\u001b[0;34m(self, word, rules)\u001b[0m\n\u001b[1;32m    264\u001b[0m                     \u001b[0;31m# Don't try any further rules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m                 \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replace_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcondition\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "df = pd.read_csv('Amazon_Unlocked_Mobile.csv')\n",
        "\n",
        "df['Reviews']=df['Reviews'].map(lambda s:preprocess(s))\n",
        "df.head()\n",
        "\n",
        "# TF-IDF VECTORIZATION\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vect = TfidfVectorizer()\n",
        "tfidf_vects = tfidf_vect.fit_transform(df['Reviews'].values.astype('U'))\n",
        "names= tfidf_vect.get_feature_names()\n",
        "\n",
        "## ELBOW METHOD\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "wcss = []\n",
        "for i in range(2,12):\n",
        "    kmeans = KMeans(n_clusters = i, init = \"k-means++\", random_state = 101)\n",
        "    kmeans.fit(tfidf_vects)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize = (11,6))\n",
        "plt.plot(range(2,12), wcss, marker = \"o\")\n",
        "plt.title (\"The Elbow Method\")\n",
        "plt.xlabel(\"Number of clusters\")\n",
        "plt.ylabel(\"WCSS\")\n",
        "\n",
        "#forming 6 clusters\n",
        "from sklearn.cluster import KMeans\n",
        "model = KMeans(n_clusters = 6,init='k-means++',max_iter=10000, random_state=50)\n",
        "model.fit(tfidf_vects)\n",
        "from collections import Counter\n",
        "Counter(model.labels_)\n",
        "\n",
        "# Clusters containing words with maximum strength\n",
        "top_words = 7\n",
        "centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
        "for cluster_num in range(6):\n",
        "    key_features = [names[i] for i in centroids[cluster_num, :top_words]]\n",
        "    print('Cluster '+str(cluster_num+1))\n",
        "    print('Top Words:', key_features)\n",
        "\n",
        "    cluster_center=model.cluster_centers_\n",
        "cluster_center\n",
        "\n",
        "reviews=[]\n",
        "for i in df['Reviews']:\n",
        "    reviews.append(str(i).split())\n",
        "import gensim\n",
        "w2v_model=gensim.models.Word2Vec(reviews, size=100, workers=4)\n",
        "\n",
        "import numpy as np\n",
        "vectors = []\n",
        "for i in reviews:\n",
        "    vector = np.zeros(100)\n",
        "    count = 0\n",
        "    for word in i:\n",
        "        try:\n",
        "            vec = w2v_model.wv[word]\n",
        "            vector += vec\n",
        "            count += 1\n",
        "        except:\n",
        "            pass\n",
        "    vector /= count\n",
        "    vectors.append(vector)\n",
        "vectors = np.array(vectors)\n",
        "vectors = np.nan_to_num(vectors)\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "minPts = 2 * 100\n",
        "# Lower bound function\n",
        "def lower_bound(nums, target):\n",
        "    l, r = 0, len(nums) - 1\n",
        "    # Binary searching\n",
        "    while l <= r:\n",
        "        mid = int(l + (r - l) / 2)\n",
        "        if nums[mid] >= target:\n",
        "            r = mid - 1\n",
        "        else:\n",
        "            l = mid + 1\n",
        "    return l\n",
        "\n",
        "def compute200thnearestneighbour(x, data):\n",
        "    dists = []\n",
        "    for val in data:\n",
        "      # computing distances\n",
        "        dist = np.sum((x - val) **2 )\n",
        "        if(len(dists) == 200 and dists[199] > dist):\n",
        "            l = int(lower_bound(dists, dist))\n",
        "            if l < 200 and l >= 0 and dists[l] > dist:\n",
        "                dists[l] = dist\n",
        "        else:\n",
        "            dists.append(dist)\n",
        "            dists.sort()\n",
        "\n",
        "# Dist 199 contains the distance of 200th nearest neighbour.\n",
        "    return dists[199]\n",
        "\n",
        "vectors.shape\n",
        "\n",
        "# Computing the 200th nearest neighbour distance of some point the dataset:\n",
        "twohundrethneigh = []\n",
        "for val in vectors[:1000]:\n",
        "    twohundrethneigh.append( compute200thnearestneighbour(val, vectors[:1000]) )\n",
        "twohundrethneigh.sort()\n",
        "\n",
        "# Plotting for the Elbow Method :\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "plt.figure(figsize=(14,4))\n",
        "plt.title(\"Elbow Method for Finding the right Eps hyperparameter\")\n",
        "plt.plot([x for x in range(len(twohundrethneigh))], twohundrethneigh)\n",
        "plt.xlabel(\"Number of points\")\n",
        "plt.ylabel(\"Distance of 200th Nearest Neighbour\")\n",
        "plt.show()\n",
        "\n",
        "# Create the model\n",
        "model_dbs = DBSCAN(eps = 5, min_samples = minPts)\n",
        "model_dbs.fit(vectors)\n",
        "\n",
        "df_dbs = df\n",
        "df_dbs[\"DBS Cluster Label\"] = model_dbs.labels_\n",
        "df_dbs\n",
        "\n",
        "import scipy\n",
        "from scipy.cluster import hierarchy\n",
        "dendro=hierarchy.dendrogram(hierarchy.linkage(vectors,method='ward'))\n",
        "plt.axhline(y=20)\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "cluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')  #took n=3 from dendrogram curve\n",
        "Agg=cluster.fit_predict(vectors)\n",
        "\n",
        "df['AVG-W2V Clus Label'] = cluster.labels_\n",
        "df.head()\n",
        "hier_df = df # Give the labels and group to count the number of data in each clusters.\n",
        "hier_df[\"Hierarchial Cluster Labels\"] = cluster.labels_\n",
        "hier_df.groupby([\"Hierarchial Cluster Labels\"])[\"Reviews\"].count()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In one paragraph, please compare the results of K-means, DBSCAN, Hierarchical clustering, Word2Vec, and BERT.**"
      ],
      "metadata": {
        "id": "tRijW2aLGONl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write your response here:**\n",
        "\n",
        ".\n",
        "K-means clustering is a popular unsupervised machine learning algorithm used for partitioning a dataset into a predetermined number of clusters. The \"K\" in K-means refers to the number of clusters you want to identify within your data. The algorithm iteratively assigns each data point to one of K clusters based on the feature similarity, aiming to minimize the within-cluster sum of squared distances from the centroid.\n",
        "# New Section\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "DBSCAN is used to perform clustering that is based on density areas where the points are concentrated the most are found, the chierarichal clustering is the derived from the name itself, it cosiders each points as cluster and finds the two clusters that are close to them\n",
        ".\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pIYCj5qyGfSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "The ram consumed is more which is leading to unexpected crashing.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}